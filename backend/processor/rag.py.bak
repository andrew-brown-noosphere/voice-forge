"""
Retrieval-Augmented Generation (RAG) system for VoiceForge.
"""
import logging
import uuid
import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re
import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from processor.chunker import ContentChunker
from processor.pinecone_rag import PineconeRAGStore

logger = logging.getLogger(__name__)

class RAGSystem:
    """
    Implementation of a Retrieval-Augmented Generation system for VoiceForge.
    Retrieves relevant content chunks and generates brand-consistent responses.
    """
    
    def __init__(self, db, embedding_model=None):
        """
        Initialize the RAG system.
        
        Args:
            db: Database interface
            embedding_model: Optional pre-initialized embedding model
        """
        self.db = db
        self.embedding_model = embedding_model
        self.chunker = ContentChunker(chunk_size=500, chunk_overlap=100)
        
        # Initialize vector store
        self.use_pinecone = os.environ.get('VECTOR_DB_PROVIDER', '').lower() == 'pinecone'
        if self.use_pinecone:
            self.vector_store = PineconeRAGStore(db)
            logger.info("Using Pinecone for RAG vector storage")
        else:
            self.vector_store = None
            logger.info("Using database for RAG vector storage")
    
    def get_embedding_model(self):
        """Lazy-load the embedding model with fallbacks."""
        if self.embedding_model is None:
            # Try to get embedding model from processor service
            try:
                from processor.service import ProcessorService
                processor_service = ProcessorService(self.db)
                self.embedding_model = processor_service.get_embedding_model()
                logger.info("Using embedding model from processor service")
            except Exception as e:
                logger.error(f"Failed to get embedding model from processor service: {e}")
                
                # Try importing directly
                try:
                    from sentence_transformers import SentenceTransformer
                    self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                    logger.info("Initialized SentenceTransformer model directly")
                except ImportError:
                    # If sentence_transformers is not available, use TF-IDF
                    logger.warning("SentenceTransformer not available, using TF-IDF embeddings")
                    from processor.service import SimpleTfIdfEmbedder
                    self.embedding_model = SimpleTfIdfEmbedder(dimension=768)
                except Exception as e:
                    logger.error(f"Failed to initialize embedding model: {e}")
                    
                    # Last resort: Create a dummy embedding model
                    logger.warning("Using random embedding model as last resort")
                    
                    class RandomEmbedder:
                        """Simple random vector generator as a last resort."""
                        def encode(self, texts, **kwargs):
                            """Generate random vectors."""
                            if isinstance(texts, str):
                                return np.random.rand(768).astype(np.float32)
                            else:
                                return np.random.rand(len(texts), 768).astype(np.float32)
                    
                    self.embedding_model = RandomEmbedder()
        
        return self.embedding_model
    
    def process_content_for_rag(self, content_id: str) -> bool:
        """
        Process content into chunks for RAG.
        
        Args:
            content_id: ID of the content to process
            
        Returns:
            Success status
        """
        try:
            # Get content from database
            content = self.db.get_content(content_id)
            if not content:
                logger.warning(f"Content {content_id} not found")
                return False
            
            # Split content into chunks
            chunks = self.chunker.process_content(content)
            logger.info(f"Generated {len(chunks)} chunks for content {content_id}")
            
            # Generate embeddings for chunks
            model = self.get_embedding_model()
            
            # Process chunks in batches to avoid memory issues
            batch_size = 32
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i+batch_size]
                texts = [chunk["text"] for chunk in batch]
                
                # Generate embeddings
                embeddings = model.encode(texts)
                
                # Update chunks with embeddings
                for j, embedding in enumerate(embeddings):
                    batch[j]["embedding"] = embedding.tolist()
                
                # Store chunks in vector database or regular database
                if self.use_pinecone and self.vector_store:
                    success = self.vector_store.store_chunks(batch)
                    if not success:
                        logger.warning(f"Failed to store chunks in Pinecone, falling back to database")
                        self.db.store_content_chunks(batch)
                else:
                    # Store chunks in database
                    self.db.store_content_chunks(batch)
            
            logger.info(f"Successfully processed content {content_id} for RAG")
            return True
            
        except Exception as e:
            logger.error(f"Failed to process content {content_id} for RAG: {str(e)}")
            return False
    
    def retrieve_relevant_chunks(
        self, 
        query: str, 
        top_k: int = 5, 
        domain: Optional[str] = None,
        content_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Retrieve the most relevant chunks for a query.
        
        Args:
            query: Search query
            top_k: Number of chunks to retrieve
            domain: Optional domain filter
            content_type: Optional content type filter
            
        Returns:
            List of relevant chunks with similarity scores
        """
        try:
            # Generate embedding for query
            model = self.get_embedding_model()
            query_embedding = model.encode(query).tolist()
            
            # Define filters
            filters = {}
            if domain:
                filters['domain'] = domain
            if content_type:
                filters['content_type'] = content_type
                
            # Get relevant chunks from vector store or database
            if self.use_pinecone and self.vector_store:
                chunks = self.vector_store.search_chunks(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    filters=filters
                )
            else:
                # Get from database
                chunks = self.db.search_chunks_by_vector(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    domain=domain,
                    content_type=content_type
                )
            
            logger.info(f"Retrieved {len(chunks)} relevant chunks for query: {query[:50]}...")
            
            # If no chunks found, try a fallback approach (text search)
            if not chunks:
                logger.warning(f"No vector-based chunks found for query: {query}, trying text-based search")
                # Try simple text search if available
                if hasattr(self.db, 'search_chunks_by_text'):
                    try:
                        chunks = self.db.search_chunks_by_text(
                            query=query,
                            top_k=top_k,
                            domain=domain,
                            content_type=content_type
                        )
                        logger.info(f"Retrieved {len(chunks)} chunks using text search")
                    except Exception as text_search_error:
                        logger.error(f"Text search fallback also failed: {str(text_search_error)}")
                
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to retrieve relevant chunks: {str(e)}")
            return []
    
    def generate_template_response(
        self, 
        query: str, 
        platform: str, 
        tone: str, 
        chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate a response using the retrieved chunks and a template.
        
        Args:
            query: Original query
            platform: Target platform (e.g., "twitter", "email")
            tone: Desired tone (e.g., "professional", "casual")
            chunks: Retrieved relevant chunks
            
        Returns:
            Generated response
        """
        try:
            # Find an appropriate template
            template = self.db.get_template(platform=platform, tone=tone)
            
            if not template:
                logger.warning(f"No template found for platform={platform}, tone={tone}")
                template = {
                    "template_text": "Here's some information about {{topic}}: {{content}}"
                }
            
            # Extract key information from chunks
            combined_text = "\n".join([chunk["text"] for chunk in chunks])
            
            # Generate simplified context (keypoints)
            key_points = self._extract_key_points(combined_text)
            
            # Fill template with information
            response_text = self._fill_template(
                template["template_text"],
                {
                    "query": query,
                    "topic": self._extract_topic(query),
                    "content": combined_text,
                    "key_points": "\n".join([f"- {point}" for point in key_points]),
                    "platform": platform,
                    "tone": tone
                }
            )
            
            # Generate response metadata
            response = {
                "text": response_text,
                "source_chunks": [
                    {
                        "chunk_id": chunk["id"],
                        "text": chunk["text"][:150] + "...",
                        "similarity": chunk["similarity"],
                        "content_id": chunk["content_id"]
                    }
                    for chunk in chunks
                ],
                "template_id": template.get("id", "default"),
                "metadata": {
                    "platform": platform,
                    "tone": tone,
                    "generated_at": datetime.utcnow().isoformat(),
                    "query": query
                }
            }
            
            logger.info(f"Generated response for query: {query[:50]}...")
            return response
            
        except Exception as e:
            logger.error(f"Failed to generate response: {str(e)}")
            return {
                "text": f"Sorry, I couldn't generate a response at this time.",
                "error": str(e),
                "metadata": {
                    "platform": platform,
                    "tone": tone,
                    "generated_at": datetime.utcnow().isoformat(),
                    "query": query
                }
            }
    
    def _extract_topic(self, query: str) -> str:
        """Extract the main topic from a query."""
        # Simple implementation - in a production system, use NLP
        words = query.split()
        if len(words) <= 3:
            return query
        
        # Use the first few words as the topic
        return " ".join(words[:3]) + "..."
    
    def _extract_key_points(self, text: str, max_points: int = 5) -> List[str]:
        """Extract key points from text."""
        # Simple implementation - in a production system, use NLP
        sentences = re.split(r'[.!?]', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        # Take the first few sentences as key points
        key_points = []
        for sentence in sentences[:max_points]:
            # Simplify sentences to make them more like bullet points
            simplified = re.sub(r'^(However|Moreover|Furthermore|Additionally|In addition|Also|Thus|Therefore|Hence)', '', sentence).strip()
            if simplified and len(simplified) > 15:
                key_points.append(simplified)
        
        return key_points
    
    def _fill_template(self, template: str, data: Dict[str, str]) -> str:
        """Fill a template with data."""
        result = template
        
        # Replace placeholders with data
        for key, value in data.items():
            placeholder = f"{{{{{key}}}}}"
            result = result.replace(placeholder, value)
        
        return result
    
    def process_and_generate(
        self,
        query: str,
        platform: str,
        tone: str,
        domain: Optional[str] = None,
        content_type: Optional[str] = None,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        End-to-end process to retrieve relevant chunks and generate a response.
        
        Args:
            query: Search query
            platform: Target platform
            tone: Desired tone
            domain: Optional domain filter
            content_type: Optional content type filter
            top_k: Number of chunks to retrieve
            
        Returns:
            Generated response with source information
        """
        # Log processing request
        logger.info(f"Processing content generation request for query: {query}")
        logger.info(f"Parameters: platform={platform}, tone={tone}, domain={domain}, content_type={content_type}")
        
        # Check if any content exists at all
        content_exists = False
        try:
            # Try to get at least one piece of content to verify database has data
            if hasattr(self.db, 'check_content_exists'):
                content_exists = self.db.check_content_exists(domain=domain, content_type=content_type)
            else:
                # Fallback implementation - safer query approach to avoid transaction issues
                try:
                    from database.models import Content
                    from sqlalchemy import func, exists
                    
                    # Create a new session for this check to avoid transaction issues
                    from database.session import get_db_session
                    temp_session = next(get_db_session())
                    
                    try:
                        # Build a query that works with filters
                        query_obj = temp_session.query(Content)
                        if domain:
                            query_obj = query_obj.filter(Content.domain == domain)
                        if content_type:
                            query_obj = query_obj.filter(Content.content_type == content_type)
                        
                        # Use count() which is more reliable than exists() in some cases
                        content_count = query_obj.count()
                        content_exists = content_count > 0
                        
                        logger.info(f"Content check found {content_count} matching contents")
                    except Exception as inner_e:
                        logger.error(f"Error in content existence check query: {str(inner_e)}")
                    finally:
                        # Always close the temporary session
                        temp_session.close()
                except Exception as outer_e:
                    logger.error(f"Failed to create temp session for content check: {str(outer_e)}")
                
            if not content_exists:
                logger.warning(f"No content found in database for domain={domain}, content_type={content_type}")
                return {
                    "text": f"No content has been crawled yet for this request. Please crawl some content first.",
                    "source_chunks": [],
                    "metadata": {
                        "platform": platform,
                        "tone": tone,
                        "generated_at": datetime.utcnow().isoformat(),
                        "query": query,
                        "error": "no_content"
                    }
                }
        except Exception as e:
            logger.error(f"Error checking content existence: {str(e)}")
        
        # Step 1: Retrieve relevant chunks
        chunks = self.retrieve_relevant_chunks(
            query=query,
            top_k=top_k,
            domain=domain,
            content_type=content_type
        )
        
        # Step 2: Generate response
        if not chunks:
            # Debug logs to help identify why no chunks were found
            logger.warning(f"No chunks found for query: {query}")
            
            # Check if any chunks exist at all - using a safer approach
            chunks_exist = False
            try:
                # Create a new session for this check to avoid transaction issues
                from database.session import get_db_session
                from database.models import ContentChunk
                from sqlalchemy import func
                
                temp_session = next(get_db_session())  # Use next() to get the session from the generator
                try:
                    chunks_count = temp_session.query(func.count(ContentChunk.id)).scalar()
                    chunks_exist = chunks_count > 0
                    logger.info(f"Chunks exist in database: {chunks_exist} (count: {chunks_count})")
                except Exception as e:
                    logger.error(f"Error checking chunks count: {str(e)}")
                finally:
                    temp_session.close()
            except Exception as e:
                logger.error(f"Error checking chunks existence: {str(e)}")
            
            error_detail = "unknown"
            if not content_exists:
                error_detail = "no_content"
            elif not chunks_exist:
                error_detail = "no_chunks"
            else:
                error_detail = "no_relevant_chunks"
                
            return {
                "text": f"Sorry, I couldn't find relevant information for '{query}'. The system may need more content to be processed for RAG.",
                "source_chunks": [],
                "metadata": {
                    "platform": platform,
                    "tone": tone,
                    "generated_at": datetime.utcnow().isoformat(),
                    "query": query,
                    "error": error_detail
                }
            }
        
        response = self.generate_template_response(
            query=query,
            platform=platform,
            tone=tone,
            chunks=chunks
        )
        
        return response
