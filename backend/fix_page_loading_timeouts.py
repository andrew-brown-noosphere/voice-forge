"""
Fix for page loading timeouts in VoiceForge crawler
Improve the page loading strategy to handle slow/complex pages
"""

# CURRENT PROBLEM in your crawler/engine.py:
# response = page.goto(url)  # Default waits for 'load' event

# IMPROVED SOLUTION:
def enhanced_page_navigation(page, url, timeout_ms=30000):
    """
    Enhanced page navigation with multiple fallback strategies
    """
    import time
    from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
    
    strategies = [
        # Strategy 1: Try networkidle (best for most pages)
        {'wait_until': 'networkidle', 'timeout': timeout_ms // 2},
        
        # Strategy 2: Try domcontentloaded (faster fallback)  
        {'wait_until': 'domcontentloaded', 'timeout': timeout_ms // 3},
        
        # Strategy 3: Try commit (minimal wait)
        {'wait_until': 'commit', 'timeout': timeout_ms // 4},
    ]
    
    last_error = None
    
    for i, strategy in enumerate(strategies):
        try:
            print(f"   üì° Trying navigation strategy {i+1}: {strategy['wait_until']}")
            
            response = page.goto(
                url, 
                wait_until=strategy['wait_until'],
                timeout=strategy['timeout']
            )
            
            # If we get here, navigation succeeded
            print(f"   ‚úÖ Navigation successful with {strategy['wait_until']}")
            return response
            
        except PlaywrightTimeoutError as e:
            last_error = e
            print(f"   ‚è±Ô∏è Strategy {i+1} timed out, trying next...")
            continue
        except Exception as e:
            last_error = e
            print(f"   ‚ùå Strategy {i+1} failed: {str(e)}")
            continue
    
    # If all strategies failed, raise the last error
    raise last_error

def get_improved_crawler_config():
    """
    Get improved crawler configuration for handling various page types
    """
    return {
        # Reliable test sites that load quickly
        "test_domains": [
            "https://httpbin.org",           # API testing site - very fast
            "https://example.com",           # Standard test domain
            "https://jsonplaceholder.typicode.com",  # JSON API - fast
            "https://httpstat.us",           # HTTP status testing - fast
        ],
        
        # Improved configuration
        "config": {
            "max_pages": 5,
            "max_depth": 2,
            "delay": 3,  # 3 seconds between requests
            "timeout": 20,  # 20 seconds per page
            "follow_external_links": False,
            "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            
            # Advanced options
            "wait_strategy": "smart",  # Use multiple fallback strategies
            "skip_slow_pages": True,   # Skip pages that take too long
            "max_page_load_time": 15   # Skip if page takes longer than 15s
        }
    }

# INTEGRATION GUIDE for your crawler/engine.py:

INTEGRATION_CODE = '''
# Replace this in your PlaywrightCrawler.crawl() method:

# OLD CODE:
# response = page.goto(url)

# NEW CODE:
try:
    response = enhanced_page_navigation(page, url, self.config.timeout * 1000)
except Exception as e:
    logger.warning(f"All navigation strategies failed for {url}: {str(e)}")
    # Skip this page and continue
    continue

# Also add after getting HTML:
if len(html) < 100:  # Skip pages with minimal content
    logger.info(f"Skipping {url} - minimal content ({len(html)} chars)")
    continue
'''

def test_page_loading_strategies():
    """
    Test different page loading strategies on various sites
    """
    print("üß™ TESTING PAGE LOADING STRATEGIES")
    print("=" * 50)
    
    test_sites = [
        ("Fast API Site", "https://httpbin.org"),
        ("Standard Test", "https://example.com"), 
        ("JSON API", "https://jsonplaceholder.typicode.com"),
        ("Complex Site", "https://github.com"),  # More complex but usually works
    ]
    
    strategies = ['networkidle', 'domcontentloaded', 'commit']
    
    for site_name, url in test_sites:
        print(f"\nüåê Testing: {site_name} ({url})")
        
        for strategy in strategies:
            try:
                import requests
                import time
                
                start_time = time.time()
                response = requests.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                })
                load_time = time.time() - start_time
                
                if response.status_code == 200:
                    print(f"   ‚úÖ {strategy}: {load_time:.2f}s ({len(response.content)} bytes)")
                else:
                    print(f"   ‚ö†Ô∏è {strategy}: Status {response.status_code}")
                    
            except Exception as e:
                print(f"   ‚ùå {strategy}: {str(e)}")

def recommend_crawler_targets():
    """
    Recommend specific sites for testing RAG automation
    """
    print(f"\n{'=' * 50}")
    print("üéØ RECOMMENDED SITES FOR RAG TESTING")
    print("=" * 50)
    
    recommendations = [
        {
            "name": "HTTPBin",
            "url": "https://httpbin.org",
            "why": "Designed for HTTP testing, guaranteed to work",
            "expected_pages": 3,
            "load_time": "< 2 seconds"
        },
        {
            "name": "Example.com", 
            "url": "https://example.com",
            "why": "Standard test domain, very simple",
            "expected_pages": 1,
            "load_time": "< 1 second"
        },
        {
            "name": "JSON Placeholder",
            "url": "https://jsonplaceholder.typicode.com", 
            "why": "REST API documentation, fast and reliable",
            "expected_pages": 4,
            "load_time": "< 3 seconds"
        },
        {
            "name": "HTTP Status",
            "url": "https://httpstat.us",
            "why": "HTTP status testing service",
            "expected_pages": 2,
            "load_time": "< 2 seconds"
        }
    ]
    
    for rec in recommendations:
        print(f"\n‚úÖ {rec['name']}")
        print(f"   URL: {rec['url']}")
        print(f"   Why: {rec['why']}")
        print(f"   Expected pages: {rec['expected_pages']}")
        print(f"   Load time: {rec['load_time']}")
    
    print(f"\nüöÄ Test your RAG automation with these sites:")
    print("1. They load quickly and reliably")
    print("2. They don't block crawlers")
    print("3. They have enough content to trigger RAG optimization")
    print("4. You'll see your automation working perfectly!")

def main():
    """
    Main function to show page loading solutions
    """
    print("‚è±Ô∏è VOICEFORGE CRAWLER - PAGE LOADING TIMEOUT FIXES")
    print("=" * 60)
    
    print("üîç CURRENT ISSUE:")
    print("   Your crawler is now reaching websites (good!)")
    print("   But some pages take too long to fully load")
    print("   SignPath.io/contact probably has heavy JavaScript")
    
    print(f"\n{'=' * 60}")
    print("üí° SOLUTIONS")
    print("=" * 60)
    
    print("1. üéØ IMMEDIATE: Test with faster sites")
    recommend_crawler_targets()
    
    print(f"\n2. üõ†Ô∏è TECHNICAL: Improve page loading strategy")
    print("   Add fallback loading strategies to handle slow pages")
    print("   Skip pages that take too long")
    print("   Use smarter wait conditions")
    
    print(f"\n3. ‚ö° QUICK FIX: Reduce timeout")
    print("   Change timeout from 30s to 15s")
    print("   Skip slow pages instead of waiting forever")
    
    test_page_loading_strategies()
    
    print(f"\n{'=' * 60}")
    print("üéâ BOTTOM LINE")
    print("=" * 60)
    print("‚úÖ Your User-Agent fix worked - no more blocking!")
    print("‚úÖ Crawler is now reaching websites successfully")
    print("‚ö†Ô∏è Some pages are just slow to load (normal)")
    print("üöÄ Test with recommended fast sites to see RAG automation!")
    
    config = get_improved_crawler_config()
    print(f"\nüìã Use this test configuration:")
    import json
    print(json.dumps({
        "domain": config["test_domains"][0],
        "config": config["config"]
    }, indent=2))
    
    return 0

if __name__ == "__main__":
    exit(main())
